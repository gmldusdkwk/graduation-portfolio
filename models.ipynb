{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('./data/X_train.npy')\n",
    "Y_train = np.load('./data/Y_train.npy')\n",
    "X_validation = np.load('./data/X_val.npy')\n",
    "Y_validation = np.load('./data/Y_val.npy')\n",
    "X_test = np.load('./data/X_test.npy')\n",
    "Y_test = np.load('./data/Y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:  -0.63853276 \n",
      "max:  0.5564938 \n",
      "mean:  -1.0648628e-05 \n",
      "median:  2.0017981e-07 \n",
      "variance:  0.005960775 \n",
      "length:  15976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb9e037588>]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 기본적인 데이터에 대한 정리 \n",
    "x, r = librosa.load(train_dir +'down3.wav', sr = 16000)\n",
    "print('min: ',np.min(x), \n",
    "      '\\nmax: ', np.max(x), \n",
    "      '\\nmean: ', np.mean(x),\n",
    "      '\\nmedian: ', np.median(x),\n",
    "      '\\nvariance: ', np.var(x),\n",
    "      '\\nlength: ', len(x))\n",
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = np.max(Y_test) + 1\n",
    "y_test_one_hot = np.eye(n_values)[Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31841, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = np.max(Y_train) + 1\n",
    "y_train_one_hot = np.eye(n_values)[Y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47857, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = np.max(Y_validation) + 1\n",
    "y_validation_one_hot = np.eye(n_values)[Y_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26533, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validation_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization, Dense\n",
    "from tensorflow.keras import optimizers, activations, models, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 122, 85, 1)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 122, 85, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 121, 84, 8)        40        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 120, 83, 8)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 60, 41, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 60, 41, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 58, 39, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 37, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 28, 18, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 18, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 16, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 8, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 8, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3328)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               426112    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                1935      \n",
      "=================================================================\n",
      "Total params: 454,019\n",
      "Trainable params: 453,505\n",
      "Non-trainable params: 514\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (122, 85, 1)\n",
    "nclass = 15\n",
    "inp = Input(shape=input_shape)\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(norm_inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Flatten()(img_1)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(img_1))\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "model = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=opt, loss=losses.binary_crossentropy)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47857 samples, validate on 26533 samples\n",
      "Epoch 1/3\n",
      "47857/47857 [==============================] - 321s 7ms/step - loss: 0.1032 - val_loss: 0.0736\n",
      "Epoch 2/3\n",
      "47857/47857 [==============================] - 300s 6ms/step - loss: 0.0628 - val_loss: 0.0501\n",
      "Epoch 3/3\n",
      "47857/47857 [==============================] - 308s 6ms/step - loss: 0.0518 - val_loss: 0.0433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10f688d30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_one_hot, batch_size=16, validation_data = (X_validation, y_validation_one_hot), epochs=3, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31841/31841 [==============================] - 45s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(X_test, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8914606953299206\n"
     ]
    }
   ],
   "source": [
    "acc = 0 \n",
    "for i in range(len(result)):\n",
    "    if(result[i].argmax() == Y_test[i]):\n",
    "        acc += 1\n",
    "print(acc / 31841)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ec53bbfc414f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# creates a HDF5 file 'my_model.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[0;31m# deletes the existing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31841/31841 [==============================] - 46s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(X_test, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8914606953299206\n"
     ]
    }
   ],
   "source": [
    "acc = 0 \n",
    "for i in range(len(result)):\n",
    "    if(result[i].argmax() == Y_test[i]):\n",
    "        acc += 1\n",
    "print(acc / 31841)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spec(file,flip = False, ps = False, st = 4):\n",
    "    \"\"\"\n",
    "    create a melspectrogram from the amplitude of the sound\n",
    "    \n",
    "    Args:\n",
    "        file (str): filename\n",
    "        file_dir (str): directory path\n",
    "        flip (bool): reverse time axis\n",
    "        ps (bool): pitch shift\n",
    "        st (int): half-note steps for pitch shift\n",
    "    Returns:\n",
    "        np.array with shape (122,85) (time, freq)\n",
    "    \"\"\"\n",
    "    sig, rate = librosa.load(file, sr = 16000)\n",
    "    if len(sig) < 16000: # pad shorter than 1 sec audio with ramp to zero\n",
    "        sig = np.pad(sig, (0,16000-len(sig)), 'linear_ramp')\n",
    "    if ps:\n",
    "        # Pitch-shift the waveform by `n_steps` half-steps. \n",
    "        sig = librosa.effects.pitch_shift(sig, rate, st)\n",
    "        \n",
    "    # Convert an amplitude spectrogram to dB-scaled spectrogram.\n",
    "    D = librosa.amplitude_to_db(librosa.stft(sig[:16000], n_fft = 512, \n",
    "                                             hop_length = 128, \n",
    "                                             center = False), ref = np.max)\n",
    "    # Compute a mel-scaled spectrogram.\n",
    "    S = librosa.feature.melspectrogram(S=D, n_mels = 85).T\n",
    "    if flip:\n",
    "        # Flip array in the up/down direction.\n",
    "        S = np.flipud(S)\n",
    "    return S.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(file):\n",
    "    import librosa\n",
    "    from tensorflow.keras.models import load_model\n",
    "    model = load_model('my_model.h5')\n",
    "    classes = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'backward', 'forward','house','silence', 'unknown']\n",
    "\n",
    "    command = make_spec(file)\n",
    "    command = np.expand_dims(command, -1)+1.3\n",
    "    command = np.expand_dims(command, 0)\n",
    "    result1 = model.predict(command, batch_size=16, verbose=1)\n",
    "    \n",
    "    return classes[result1.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/librosa/core/spectrum.py:960: UserWarning: amplitude_to_db was called on complex input so phase information will be discarded. To suppress this warning, call amplitude_to_db(np.abs(S)) instead.\n",
      "  warnings.warn('amplitude_to_db was called on complex input so phase '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'silence'"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "command = make_spec('command.wav')\n",
    "command = np.expand_dims(command, -1)+1.3\n",
    "command = np.expand_dims(command, 0)\n",
    "result1 = model.predict(command, batch_size=16, verbose=1)\n",
    "classes[result1.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['yes', 'no', \n",
    "           'up', 'down', \n",
    "           'left', 'right', \n",
    "           'on', 'off', \n",
    "           'stop', 'go', \n",
    "           'backward', 'forward',\n",
    "           'house',\n",
    "           'silence', 'unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = np.load('data/Xtrain.npy')\n",
    "ytrain = np.load('data/ytrain.npy')\n",
    "Xtest = np.load('data/Xtest.npy')\n",
    "ytest = np.load('data/ytest.npy')\n",
    "Xval = np.load('data/Xval.npy')\n",
    "yval = np.load('data/yval.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = np.max(ytrain) + 1\n",
    "ytrainonehot = np.eye(n_values)[ytrain]\n",
    "n_values = np.max(yval) + 1\n",
    "yvalonehot = np.eye(n_values)[yval]\n",
    "n_values = np.max(ytest) + 1\n",
    "ytestonehot = np.eye(n_values)[ytest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 122, 85, 1)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 122, 85, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 121, 84, 8)        40        \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 120, 83, 8)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 60, 41, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 60, 41, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 58, 39, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 56, 37, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 28, 18, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 28, 18, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 26, 16, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 13, 8, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 13, 8, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 3328)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               426112    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 453,761\n",
      "Trainable params: 453,247\n",
      "Non-trainable params: 514\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (122, 85, 1)\n",
    "nclass = 13\n",
    "inp = Input(shape=input_shape)\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(norm_inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Flatten()(img_1)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(img_1))\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "model2 = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "model2.compile(optimizer=opt, loss=losses.binary_crossentropy)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13154 samples, validate on 10963 samples\n",
      "Epoch 1/3\n",
      "13154/13154 [==============================] - 104s 8ms/step - loss: 0.1726 - val_loss: 0.2278\n",
      "Epoch 2/3\n",
      "13154/13154 [==============================] - 98s 7ms/step - loss: 0.1034 - val_loss: 0.1297\n",
      "Epoch 3/3\n",
      "13154/13154 [==============================] - 105s 8ms/step - loss: 0.0823 - val_loss: 0.0814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb9d9214e0>"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(Xtrain, ytrainonehot, batch_size=16, validation_data = (Xval, yvalonehot), epochs=3, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13154/13154 [==============================] - 23s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "result = model2.predict(Xtest, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3502716623221632\n"
     ]
    }
   ],
   "source": [
    "acc = 0 \n",
    "for i in range(len(result)):\n",
    "    if(result[i].argmax() == ytest[i]):\n",
    "        acc += 1\n",
    "print(acc / 31841)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
